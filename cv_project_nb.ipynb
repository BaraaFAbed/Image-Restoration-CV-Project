{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cd85467-ad11-40a6-acb8-719bbc09fc89",
   "metadata": {},
   "source": [
    "# **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcf64b1-0842-4efa-a5f9-2969ec7d6ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from typing import List\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import Markdown\n",
    "from torch.utils.data import random_split\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c348e09-5d22-40d6-8a6e-c4787cf23592",
   "metadata": {},
   "source": [
    "# **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b77a3a9-f8b2-4534-8559-684ac69f7b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoProDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, target_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Transform to be applied on the input images.\n",
    "            target_transform (callable, optional): Transform to be applied on the target images.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.samples = []\n",
    "\n",
    "        # Iterate over each sequence folder\n",
    "        for sequence in os.listdir(root_dir):\n",
    "            sequence_dir = os.path.join(root_dir, sequence)\n",
    "            blur_dir = os.path.join(sequence_dir, 'blur')\n",
    "            sharp_dir = os.path.join(sequence_dir, 'sharp')\n",
    "\n",
    "            # List and sort blur and sharp images to ensure they are aligned\n",
    "            blur_images = sorted([os.path.join(blur_dir, f) for f in os.listdir(blur_dir) if f.endswith('.png')])\n",
    "            sharp_images = sorted([os.path.join(sharp_dir, f) for f in os.listdir(sharp_dir) if f.endswith('.png')])\n",
    "\n",
    "            # Pair each blur image with its corresponding sharp image\n",
    "            self.samples.extend(zip(blur_images, sharp_images))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        blur_path, sharp_path = self.samples[idx]\n",
    "        blur_image = Image.open(blur_path)\n",
    "        sharp_image = Image.open(sharp_path)\n",
    "\n",
    "        if self.transform:\n",
    "            blur_image = self.transform(blur_image)\n",
    "        if self.target_transform:\n",
    "            sharp_image = self.target_transform(sharp_image)\n",
    "\n",
    "        return blur_image, sharp_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faf6cd7-c2a7-4823-b184-163544e9c93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop((540,960)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "target_transform = transforms.Compose([\n",
    "    transforms.CenterCrop((540,960)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = GoProDataset(root_dir='GoPro/train', transform=transform, target_transform=target_transform)\n",
    "test_dataset = GoProDataset(root_dir='GoPro/test', transform=transform, target_transform=target_transform)\n",
    "\n",
    "# Define the size of the validation set\n",
    "val_size = int(0.2 * len(train_dataset))  # 20% of the training dataset for validation\n",
    "\n",
    "# Split train_dataset into train and validation sets\n",
    "train_size = len(train_dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders for training, validation, and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d852bd0d-3881-495b-8cb9-17f45275d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "# Iterate over data and display images\n",
    "for blur_images, sharp_images in train_loader:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(121)\n",
    "    imshow(blur_images[0])\n",
    "    plt.title('Blur Image')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    imshow(sharp_images[0])\n",
    "    plt.title('Sharp Image')\n",
    "    plt.show()\n",
    "\n",
    "    # Optionally, break after the first pair is shown\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9011ae-b3a4-4a0b-9d98-1de70d806ece",
   "metadata": {},
   "source": [
    "# **Utility Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80536647-4c77-4c72-b542-69e3d44d43bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_psnr(img1, img2):\n",
    "    return 10. * torch.log10(1. / torch.mean((img1-img2)**2))\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum/self.count\n",
    "\n",
    "def weights_init_xavier(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(m.weight.data)\n",
    "        m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8102e628-55fb-45c3-8901-d378525cf272",
   "metadata": {},
   "source": [
    "# **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e0331b-2445-40db-9dac-05fb3b214f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self._spatial_attention_conv = nn.Conv2d(2, dim, kernel_size=3, padding=1)\n",
    "\n",
    "        # Channel attention MLP\n",
    "        self._channel_attention_conv0 = nn.Conv2d(1, dim, kernel_size=1, padding=0)\n",
    "        self._channel_attention_conv1 = nn.Conv2d(dim, dim, kernel_size=1, padding=0)\n",
    "\n",
    "        self._out_conv = nn.Conv2d(2 * dim, dim, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if len(x.shape) != 4:\n",
    "            raise ValueError(f\"Expected [B, C, H, W] input, got {x.shape}.\")\n",
    "\n",
    "        # Spatial attention\n",
    "        mean = torch.mean(x, dim=1, keepdim=True)  # Mean/Max on C axis\n",
    "        max, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        spatial_attention = torch.cat([mean, max], dim=1)  # [B, 2, H, W]\n",
    "        spatial_attention = self._spatial_attention_conv(spatial_attention)\n",
    "        spatial_attention = torch.sigmoid(spatial_attention) * x\n",
    "\n",
    "        # Channel attention. TODO: Correct that it only uses average pool contrary to CBAM?\n",
    "        # NOTE/TODO: This differs from CBAM as it uses Channel pooling, not spatial pooling!\n",
    "        # In a way, this is 2x spatial attention\n",
    "        channel_attention = torch.relu(self._channel_attention_conv0(mean))\n",
    "        channel_attention = self._channel_attention_conv1(channel_attention)\n",
    "        channel_attention = torch.sigmoid(channel_attention) * x\n",
    "\n",
    "        attention = torch.cat([spatial_attention, channel_attention], dim=1)  # [B, 2*dim, H, W]\n",
    "        attention = self._out_conv(attention)\n",
    "        return x + attention\n",
    "\n",
    "\n",
    "# TODO: This is not named in the paper right?\n",
    "# It is sort of the InverseResidualBlock but w/o the Channel and Spatial Attentions and without another Conv after ReLU\n",
    "class InverseBlock(nn.Module):\n",
    "    def __init__(self, input_channels: int, channels: int):\n",
    "        super(InverseBlock, self).__init__()\n",
    "\n",
    "        self._conv0 = nn.Conv2d(input_channels, channels, kernel_size=1)\n",
    "        self._dw_conv = nn.Conv2d(channels, channels, kernel_size=3, padding=1, groups=channels)\n",
    "        self._conv1 = nn.Conv2d(channels, channels, kernel_size=1)\n",
    "        self._conv2 = nn.Conv2d(input_channels, channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        features = self._conv0(x)\n",
    "        features = F.elu(self._dw_conv(features))  # TODO: Paper is ReLU, authors do ELU\n",
    "        features = self._conv1(features)\n",
    "\n",
    "        # TODO: The BaseBlock has residuals and one path of convolutions, not 2 separate paths - is this different on purpose?\n",
    "        x = torch.relu(self._conv2(x))\n",
    "        return x + features\n",
    "\n",
    "\n",
    "class BaseBlock(nn.Module):\n",
    "    def __init__(self, channels: int):\n",
    "        super(BaseBlock, self).__init__()\n",
    "\n",
    "        self._conv0 = nn.Conv2d(channels, channels, kernel_size=1)\n",
    "        self._dw_conv = nn.Conv2d(channels, channels, kernel_size=3, padding=1, groups=channels)\n",
    "        self._conv1 = nn.Conv2d(channels, channels, kernel_size=1)\n",
    "\n",
    "        self._conv2 = nn.Conv2d(channels, channels, kernel_size=1)\n",
    "        self._conv3 = nn.Conv2d(channels, channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        features = self._conv0(x)\n",
    "        features = F.elu(self._dw_conv(features))  # TODO: ELU or ReLU?\n",
    "        features = self._conv1(features)\n",
    "        x = x + features\n",
    "\n",
    "        features = F.elu(self._conv2(x))\n",
    "        features = self._conv3(features)\n",
    "        return x + features\n",
    "\n",
    "\n",
    "class AttentionTail(nn.Module):\n",
    "    def __init__(self, channels: int):\n",
    "        super(AttentionTail, self).__init__()\n",
    "\n",
    "        self._conv0 = nn.Conv2d(channels, channels, kernel_size=7, padding=3)\n",
    "        self._conv1 = nn.Conv2d(channels, channels, kernel_size=5, padding=2)\n",
    "        self._conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        attention = torch.relu(self._conv0(x))\n",
    "        attention = torch.relu(self._conv1(attention))\n",
    "        attention = torch.sigmoid(self._conv2(attention))\n",
    "        return x * attention\n",
    "\n",
    "\n",
    "class LPIENet(nn.Module):\n",
    "    def __init__(self, input_channels: int, output_channels: int, encoder_dims: List[int], decoder_dims: List[int]):\n",
    "        super(LPIENet, self).__init__()\n",
    "\n",
    "        if len(encoder_dims) != len(decoder_dims) + 1 or len(decoder_dims) < 1:\n",
    "            raise ValueError(f\"Unexpected encoder and decoder dims: {encoder_dims}, {decoder_dims}.\")\n",
    "\n",
    "        if input_channels != output_channels:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        # TODO: We will need an explicit decoder head, consider Unshuffle & Shuffle\n",
    "\n",
    "        encoders = []\n",
    "        for i, encoder_dim in enumerate(encoder_dims):\n",
    "            input_dim = input_channels if i == 0 else encoder_dims[i - 1]\n",
    "            encoders.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(input_dim, encoder_dim, kernel_size=3, padding=1),\n",
    "                    BaseBlock(encoder_dim),  # TODO: one or two base blocks?\n",
    "                    BaseBlock(encoder_dim),\n",
    "                    AttentionBlock(encoder_dim),\n",
    "                )\n",
    "            )\n",
    "        self._encoders = nn.ModuleList(encoders)\n",
    "\n",
    "        decoders = []\n",
    "        for i, decoder_dim in enumerate(decoder_dims):\n",
    "            input_dim = encoder_dims[-1] if i == 0 else decoder_dims[i - 1] + encoder_dims[-i - 1]\n",
    "            decoders.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(input_dim, decoder_dim, kernel_size=3, padding=1),\n",
    "                    BaseBlock(decoder_dim),\n",
    "                    BaseBlock(decoder_dim),\n",
    "                    AttentionBlock(decoder_dim),\n",
    "                )\n",
    "            )\n",
    "        self._decoders = nn.ModuleList(decoders)\n",
    "\n",
    "        self._inverse_bock = InverseBlock(encoder_dims[0] + decoder_dims[-1], output_channels)\n",
    "        self._attention_tail = AttentionTail(output_channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if len(x.shape) != 4:\n",
    "            raise ValueError(f\"Expected [B, C, H, W] input, got {x.shape}.\")\n",
    "        global_residual = x\n",
    "\n",
    "        encoder_outputs = []\n",
    "        for i, encoder in enumerate(self._encoders):\n",
    "            x = encoder(x)\n",
    "            if i != len(self._encoders) - 1:\n",
    "                encoder_outputs.append(x)\n",
    "                x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "        for i, decoder in enumerate(self._decoders):\n",
    "            x = decoder(x)\n",
    "            x = F.interpolate(x, scale_factor=2, mode=\"bilinear\")\n",
    "            x = torch.cat([x, encoder_outputs.pop()], dim=1)\n",
    "\n",
    "        x = self._inverse_bock(x)\n",
    "        x = self._attention_tail(x)\n",
    "        return x + global_residual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74cbd59-6ae5-4bec-8985-4100e1c0a6d1",
   "metadata": {},
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44af7ede-4b9e-45bd-80d3-f727541927ff",
   "metadata": {},
   "source": [
    "## **Training loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4588f2c0-c60b-4e67-9e36-be0a007ac25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to GPU if available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Connected to {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f40f29-89be-49aa-b73b-1775711535b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LPIENet(3, 3, [16, 32, 64], [32, 16])\n",
    "model.apply(weights_init_xavier)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedfff0f-d8dc-49d1-8b34-1f637e1fe9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputDIR = 'Saves/Model_Saves/'\n",
    "\n",
    "if not os.path.exists(outputDIR):\n",
    "    os.makedirs(outputDIR)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "torch.manual_seed(42)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001) # Edit learning rate here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e5db1a-33e4-43e4-a8c4-f6b3faf7c7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_weights = copy.deepcopy(model.state_dict())\n",
    "best_epoch = 0\n",
    "best_psnr = 0.0\n",
    "num_epochs = 300 # Edit number of epochs here\n",
    "batch_size = 4\n",
    "psnrs = []\n",
    "train_history = []\n",
    "val_history = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0a308a-f9cb-4b95-9897-6eba688f07c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_losses = AverageMeter()\n",
    "\n",
    "    with tqdm(total=(len(train_dataset) - len(train_dataset) % batch_size)) as t:\n",
    "        t.set_description('epoch: {}/{}'.format(epoch+1, num_epochs))\n",
    "        train_loss = 0.0\n",
    "        for data in train_loader:\n",
    "            inputs, labels = data\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            preds = model(inputs)\n",
    "\n",
    "            loss = criterion(preds, labels)\n",
    "\n",
    "            epoch_losses.update(loss.item(), len(inputs))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss+=loss.item()\n",
    "\n",
    "            t.set_postfix(loss='{:.6f}'.format(epoch_losses.avg))\n",
    "            t.update(len(inputs))\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(outputDIR, 'epoch_{}.pth'.format(epoch)))\n",
    "\n",
    "    model.eval()\n",
    "    epoch_psnr = AverageMeter()\n",
    "    val_loss = 0\n",
    "    for data in val_loader:\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = model(inputs)\n",
    "            loss = criterion(preds, labels)\n",
    "            val_loss += loss.item()\n",
    "            preds = preds.clamp(0.0, 1.0)\n",
    "\n",
    "        epoch_psnr.update(calc_psnr(preds, labels), len(inputs))\n",
    "\n",
    "    print('train loss: {:.6f}'.format(train_loss/len(train_dataset)))\n",
    "    print('eval loss: {:.6f}'.format(val_loss/len(val_dataset)))\n",
    "    print('eval psnr: {:.2f}'.format(epoch_psnr.avg))\n",
    "    psnrs.append(epoch_psnr.avg.cpu())\n",
    "    train_history += [train_loss/len(train_dataset)]\n",
    "    val_history += [val_loss/len(val_dataset)]\n",
    "\n",
    "    if epoch_psnr.avg > best_psnr:\n",
    "        best_epoch = epoch\n",
    "        best_psnr = epoch_psnr.avg\n",
    "        best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "print('best epoch: {}, psnr: {:.2f}'.format(best_epoch, best_psnr))\n",
    "torch.save(best_weights, os.path.join(outputDIR,'best.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032a8b4f-2995-4bad-a3b5-68e81d2de204",
   "metadata": {},
   "source": [
    "## **Training Plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ff7930-9ee6-4592-b22d-7e5870f0614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_history, 'b',label=\"trianing loss\")\n",
    "plt.plot(val_history, 'r',label = \"validation loss\")\n",
    "plt.title('Convergence plot of gradient descent')\n",
    "plt.xlabel('Epoch No')\n",
    "plt.ylabel('J')\n",
    "plt.legend()\n",
    "plt.savefig('Saves/Graphs/loss_graph.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5bef76-3156-4ad9-b200-5133b12cf8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(psnrs)\n",
    "plt.title('Convergence plot of PSNR')\n",
    "plt.xlabel('Epoch No')\n",
    "plt.ylabel('PSNR')\n",
    "plt.savefig('Saves/Graphs/psnrs_graph.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba66f435-cc5e-4efc-aacc-13897ed5e0a7",
   "metadata": {},
   "source": [
    "## **Array Saves**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f598f0b-ba42-4aa5-b2a5-a1780118b978",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_psnrs = np.array(psnrs)\n",
    "np_train_history = np.array(train_history)\n",
    "np_val_history = np.array(val_history)\n",
    "\n",
    "np.save('Saves/Arrays/psnrs.npy', np_psnrs)\n",
    "np.save('Saves/Arrays/val_history.npy',np_val_history)\n",
    "np.save('Saves/Arrays/train_history.npy',np_train_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ad63bc-133e-48c1-b86a-e89e15c73d22",
   "metadata": {},
   "source": [
    "# **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e72cf65-bbf3-4fd5-a4bc-ba101670725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
